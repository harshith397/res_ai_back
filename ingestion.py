import os
from sqlalchemy import make_url
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Document
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Initialize the local embedding model (runs on CPU/GPU, no API costs)
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

# Your database connection string
DB_URL = "postgresql://admin:24072006@localhost:5432/res_ai"

def process_and_embed_pdf(file_path: str, workspace_id: str, paper_id: str):
    url = make_url(DB_URL)
    
    vector_store = PGVectorStore.from_params(
        database=url.database,
        host=url.host,
        password=url.password,
        port=url.port,
        user=url.username,
        table_name="workspace_embeddings",
        embed_dim=384 
    )
    
    # Read the physical PDF
    raw_documents = SimpleDirectoryReader(input_files=[file_path]).load_data()
    
    # SANITIZATION AND REBUILDING LOOP
    clean_documents = []
    
    for doc in raw_documents:
        # 1. Extract and clean the text safely
        clean_text = doc.text.replace('\x00', '') if doc.text else ""
        
        # 2. Instantiate a completely new Document object to bypass Pydantic immutability
        new_doc = Document(
            text=clean_text,
            metadata={
                **doc.metadata,  # Keep any original metadata generated by the PDF reader
                "workspace_id": str(workspace_id),
                "paper_id": str(paper_id)
            },
            excluded_llm_metadata_keys=["workspace_id", "paper_id"]
        )
        clean_documents.append(new_doc)
        
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    # Pass the fresh, clean_documents list to the indexer
    index = VectorStoreIndex.from_documents(
        clean_documents,
        storage_context=storage_context,
        embed_model=embed_model,
        show_progress=True 
    )
    
    return True